Pour savoir comment évolue une température, la première chose est… de pouvoir la mesurer. Cela semble évident, mais c’est pourtant une sérieuse limitation quand on parle d’évolution de la température moyenne au cours du temps, car une moyenne n’a de sens que si on dispose de suffisamment de points de mesure, et qu’en outre la mesure est toujours effectuée de la même manière, pour éviter les différences simplement liées au fait que l’on a changé de méthode.
Selon la période à laquelle on s’intéresse il y a différents moyens pour mesurer ou reconstituer les températures.
La mesure directe avec un thermomètre ayant une fiabilité acceptable n’est possible que depuis 1860 : les mesures directes de température ne permettent donc pas un recul supérieur à un siècle et demi. En outre, à cette époque (1850), et même pendant un certain temps après, les stations de mesure au sol étaient peu nombreuses, et l’océan, en particulier, était très mal couvert.
Enfin la manière de mesurer la température n’était pas nécessairement la même selon les endroits. Si l’on reprend l’exemple de l’océan, et disons que nous sommes en 1900, tel bateau pouvait mesurer la température de l’eau contenue dans un seau remonté par dessus le bord, alors que tel autre mesurait cette température dans le tuyau d’alimentation de la chaudière à vapeur : le résultat n’était pas nécessairement le même !
Il faut donc « corriger » les différentes séries disponibles, pour les rendre cohérentes les unes avec les autres. Pour reprendre l’exemple ci-dessus, si un même bateau a mesuré la température des deux manières différentes, et constaté un écart constant selon la manière de mesurer, il est alors possible d’harmoniser les deux types de mesures, en « corrigeant » les valeurs obtenues avec la méthode 1 pour les rendre comparables avec celles effectuées avec la méthode 2.
De telles « corrections » sont aussi nécessaires lorsque l’on déplace une station de mesure au sol (par exemple en périphérie d’une ville), lorsque l’on change d’appareil de mesure, ou encore d’heure de mesure de la température dans la journée…
Si l’on reporte la moyenne de ces températures mesurées, une fois harmonisées de manière adéquate, on obtient la courbe ci-dessous, qui est déjà une première indication.
  Evolution des températures moyennes de l’air au niveau du sol depuis 1850, et tendances.
L’axe vertical de gauche représente l’écart à la moyenne pour la température planétaire durant la période 1961-1990 (il s’agit en fait d’une double moyenne, géographique sur la totalité de la surface planétaire, et temporelle, sur 30 ans).
L’axe de droite représente la valeur absolue de la température planétaire (qui est donc de 14,5 °C environ actuellement).
Les points noirs donnent les valeurs annuelles (ou plus exactement la meilleure estimation de la valeur annuelle), ce qui signifie par exemple que l’année 1860 a probablement été plus froide d’environ 0,35 °C que la moyenne 1961-1990, cet écart valant 0,55 °C pour 1861, etc.
La courbe bleue au centre donne la moyenne glissante de ces valeurs sur 10 ans, afin de visualiser plus facilement l’évolution, et la zone grisée qui entoure cette courbe donne la barre d’erreur (intervalle de confiance à 95%).
Les droites de couleur donnent la tendance – linéaire – sur les durées indiquées, avec l’augmentation moyenne par décennie qui figure dans la légende. On notera que cette augmentation moyenne par décennie accélère nettement avec le temps.
La baisse et la hausse très modérée qui ont suivi 1945 sont peut-être dues à la pollution locale intense – cette dernière est un facteur de refroidissement – qui a pris place pendant les « Trente Glorieuses », période d’intense activité industrielle.
Source: GIEC, 4th assessment report, 2007.
Ce sont toutefois les raisons rappelées plus haut qui expliquent que l’élévation de température, qualitativement très nette, est donnée avec une marge d’erreur pour sa valeur exacte : la température moyenne de l’air au niveau du sol est considérée comme ayant augmenté de 0,75 °C ± 0,2 °C depuis le début du 20è siècle. Cette courbe permet aussi de constater que les années les plus chaudes du XXè siècle sont toutes celles de la dernière décennie, et cela a continué au début du 21è siècle.
Evolution des températures moyennes de l’air au niveau du sol depuis 1880, et moyenne glissante sur 5 ans.
Source : GISS/NASA, mars 2012
Les observations permettent aussi de détailler la répartition régionale de cette élévation pour le dernier quart de siècle.
Augmentations moyennes de température par décennie de 1979 à 2005, en fonction de la région, pour la surface terrestre (à gauche) et l’ensemble de la troposphère, c’est-à-dire en gros les 10 premiers km de l’atmosphère à partir du sol (à droite).
Les zones grisées correspondent à des endroits où il n’y a pas assez de données, en gros les pôles et les océans polaires (attention à ne pas confondre les zones grises avec les zones colorées en bleu clair !).
On retrouve (déjà ?) dans ces tendances des 25 dernières années des caractéristiques prédites pour le changement futur :
élévation plus rapide sur les continents que sur les océans, l’inertie thermique de ces derniers étant plus importante,
élévation plus rapide quand on gagne en latitude en partant de l’Equateur, et en particulier élévation globalement moins rapide à l’Equateur que près des pôles et élévation particulièrement marqué dans l’Arctique (du moins là où il y a des séries de disponibles),
élévation plus rapide dans l’hémisphère Nord que dans l’hémisphère Sud (d’une part parce que ce dernier est plus océanique, et d’autre part parce que la fonte de la glace de mer augmente la part du rayonnement solaire absorbé et accélère le réchauffement),
Source : GIEC, 4th assessment report, 2007.
D’autres observations récentes sont aussi disponibles:
Exactement comme cela est attendu pour l’avenir, les températures ont plus augmenté la nuit que le jour au-dessus des continents,
Evolution des moyennes du maximum, minimum et de l’écart journalier des températures au-dessus des continents, pour 71% des terres émergées.
L’axe vertical de gauche représente l’écart à la moyenne pour la période 1961-1990.
En haut, évolution de la moyenne du minimum journalier pour 71% des terres émergées (c’est donc la double moyenne, à la fois annuelle et géographique, des minima quotidiens),
Au milieu, évolution de la moyenne du maximum quotidien pour la même zone (71% des terres émergées).
En bas, évolution de l’écart journalier – là aussi en moyenne planétaire sur l’année – depuis 1950, sur ces mêmes 71% des terres émergées.
L’amplitude thermique a diminué d’un petit demi-degré depuis 1950, ce qui signifie que, « en moyenne », l’écart s’est resserré entre la température la plus basse et la plus élevée de la journée un peu partout à la surface de la planète .
Source : GIEC, 4th assessment report, 2007.
Les précipitations ont varié, de manière contrastée, mais là aussi en accord global avec ce qui est attendu pour le 21è siècle,
Tendances annuelles de précipitations de 1901 à 2005
La carte centrale donne les variations sur le siècle et en pourcentage (100% signifie donc un doublement des précipitations, -100% un assèchement total) des précipitations par zone géographique. Les zones grisées correspondent à une absence de données suffisantes pour pouvoir établir une tendance.
Les séries temporelles des autres graphiques (d’où partent les flèches rouges) sont des évolutions régionales depuis 1900, exprimées en % d’écart à la moyenne 1961-1990, pour la zone mentionnée sur le titre (la valeur de cette moyenne est donnée à côté du titre, par exemple l’Australie du Sud a des précipitations annuelles moyennes de 580 mm).
On note une tendance à l’augmentation sur les zones très au Nord (Europe du Nord, Alaska, Asie du Nord…), une tendance à l’assèchement du bassin Méditerranéen ou de l’Asie du Sud, et bien sûr de nombreuses zones où pour l’heure « ce n’est pas clair ».
Source : GIEC, 4th assessment report, 2007.
Ceci entraînant cela, la sécheresse des sols a aussi varié (ce qui est un point majeur pour l’avenir des écosystèmes):
Evolution de 1900 à 2005 de l’index utilisé pour évaluer l’état de sécheresse d’une région, l’index Palmer (en anglais : PDSI, pour Palmer Drough Severity Index).
Le sol des régions en jaune et rouge s’est asséché, le sol des régions en bleu et vert humidifié.
Source : GIEC, 4th assessment report, 2007.
L’étendue de la glace de mer de l’hémisphère nord a reculé en été, de manière très significative:
Evolution, de 1970 à 2011, de l’étendue de la glace de mer dans l’océan Arctique lors du minimum estival.
On note une nette accélération depuis 2000 en gros.
Source : Université de Brême.
Et avant?
Avant les thermomètres, on peut aussi reconstituer les températures qui régnaient à la surface de la Terre, plus loin dans le temps. Comme on ne peut pas les mesurer directement, on les déduit d’autres observations. Ces observations doivent permettre une reconstitution de la température, bien sûr, mais aussi une reconstitution de la date, ce qui est indispensable pour savoir quand la température mesurée est survenue.
Il y a 3 grandes familles de mesures pour parvenir à ce résultat, qui portent sur des époques différentes et permettent d’obtenir des résultats différents.
Les troncs d’arbres
La première méthode fait appel aux troncs d’arbres. En analysant les cernes des troncs d’arbres (les cernes sont ce que l’on appelle aussi les « anneaux », bien visibles quand on coupe un tronc d’arbre) pour les périodes récentes (les derniers 150 ans), pour lesquels nous disposons également d’enregistrements de température et de pluviométrie fiables, les chercheurs (dont la spécialité s’appelle la dendrochronologie) se sont rendus compte que la température moyenne sur une large zone était corrélée soit à la largeur des cernes, soit à leur densité. En particulier la densité de la partie de la cerne qui pousse le plus tard en saison est assez bien corrélée à la température moyenne qu’il fait au moment du printemps et de l’été de la même année.
Les informations obtenues par cette méthode sont toutefois « perturbées » par deux effets :
les arbres poussent mieux jeunes que vieux, et donc les cernes vont en décroissant avec l’âge, quelles que soient les conditions climatiques par ailleurs,
la croissance des arbres est favorisée par la hausse de la concentration atmosphérique en CO2, qui est un phénomène récent.
Il faut donc « retirer » des séries mesurées ces biais dus à l’âge de l’arbre ou à la hausse de la teneur en CO2 de l’atmosphère. On fait cela en supprimant, par un traitement mathématique approprié, l’information sur les variations lentes des séries mesurées. Mais en faisant cela, on perd aussi, en même temps, l’information sur l’évolution lente du climat, qui se produit au même rythme que la croissance de l’arbre ou l’augmentation de la concentration en CO2. Il existe apparemment d’autres méthodes, plus récentes, qui permettent de « retrouver » les évolutions lentes du climat, mais dans tous les cas de figure ces traitement mathématiques des données brutes entachent bien sur le résultat d’une marge d’erreur plus ou moins importante.
Par contre, l’une des grandes forces de l’analyse des cernes d’arbres est qu’elle permet de dater avec précision l’année de la mesure (en comptant les cernes), et on peut le faire sur des arbres morts aussi bien que vivants, car on peut « rabouter » des troncs de différents âges en faisant correspondre les séries de cernes épaisses et minces, dont la succession se reconnaît d’un tronc à l’autre.
Cela étant, par nature même cette méthode donne surtout les variations des températures estivales et printanières, qui ne sont pas nécessairement bien représentatives des variations annuelles. C’est typiquement le cas actuellement : dans l’hémisphère Nord, les températures hivernales grimpent aujourd’hui plus vite que les températures estivales.
Différents auteurs sont néanmoins parvenus à différentes estimations de l’évolution de la température sur les 1000 dernières années, abstraction faite des décennies les plus récentes, bien sûr.
Reconstitution de la température annuelle moyenne de l’hémisphère Nord depuis l’an 700.
L’axe horizontal représente la date, et l’axe vertical représente l’écart de la valeur de l’année considérée avec la moyenne hémisphérique (nord) pour les années 1961 à 1990.
Les différents auteurs et les dates de publication des résultats sont mentionnées en haut du graphique.
Les températures observées (qui correspondent à celles du premier graphique, en haut de cette page) sont également reportées, en noir, à partir de 1860.
On constate que les évolutions sont qualitativement convergentes et cohérentes avec ce que nous savons par ailleurs du climat à ces époques, à travers les récits historiques et les analyses des documents anciens (gravures, actes administratifs par exemple) :
on retrouve la trace de l' »optimum médiéval », qui correspond à une période un peu plus chaude – et donc particulièrement propice pour l’agriculture, mais pas aussi chaude qu’aujourd’hui, et surtout à ne pas confondre avec le réchauffement massif que nous allons peut-être connaître! – que le reste du Moyen âge, aux alentours de l’an mil (c’est en 982 que Erik Le Rouge colonise le Groenland, où l’agriculture est rendue possible par la hausse des températures). Toutefois l’élévation de température est loin d’avoir été homogène sur l’ensemble du globe ; elle semble avoir été surtout marquée sur l’Atlantique Nord et ses rives.
on retrouve également la trace du « petit âge glaciaire », qui a culminé au XVIIè siècle, pendant lequel les températures se sont refroidies significativement en Europe, mais les différents auteurs ne trouvent pas le même refroidissement pour cette période (mais là aussi le débat continue pour savoir si ce refroidissement a concerné l’ensemble de la planète ; il semble bien que ce refroidissement a là aussi été particulièrement marqué sur l’Atlantique Nord).
enfin la hausse depuis 1900 est bien visible, tout comme le refroidissement temporaire de l’après guerre (la seconde, pas la première !).
Source : GIEC, 4th assessment report, 2007.
Cette méthode portant sur les troncs d’arbres permet difficilement de remonter très loin : les arbres les plus vieux datent d’il y a quelques milliers d’années (ce n’est déjà pas si mal : je ne suis pas sûr de vivre aussi longtemps !), et les arbres morts ont une fâcheuse tendance à se décomposer et à ne pas attendre sagement les chercheurs dans un parfait état de conservation. Or moins on a d’échantillons, et moins la reconstitution est précise. Cette méthode est donc limitée, en pratique, à une analyse du passé proche : quelques milliers d’années.
Les coraux
La deuxième méthode fait appel aux coraux. En effet, tous comme les arbres ont une croissance qui dépend de la température, les coraux ont une croissance qui dépend de la température de l’eau de mer. Par ailleurs on peut dater l’échantillon mesuré soit avec des cernes, soit avec des analyses isotopiques (voir encadré).
Qu’est-ce que l’analyse isotopique?
Ce qui caractérise chaque élément présent à la surface de la terre (le fer, le carbone, l’oxygène, l’hydrogène…), c’est le nombre de protons dans les atomes qui le composent, que l’on appelle encore numéro atomique. Par exemple l’hydrogène est fait d’atomes qui ne comportent qu’un proton, le carbone est fait d’atomes qui comportent 6 protons, jamais un de plus ni un de moins (sinon on change d’élément), etc.
D’aucuns se rappelleront peut-être du tableau de Mendéleïev, qui décrit à quel élément on a affaire selon le nombre de protons dans l’atome (le nombre de protons est aussi le nombre d’électrons, je rappelle !).
Mais chaque élément possède, en général, plusieurs isotopes. Les différents isotopes possèdent la même quantité de protons (sinon ce serait d’autres éléments !), et donc la même quantité d’électrons (ce qui fait qu’ils ont les mêmes propriétés chimiques, car la chimie ce n’est que de l’échange d’électrons), mais un nombre variable de neutrons, et donc des propriétés atomiques un peu différentes. Par exemple tout atome de carbone possède 6 protons, mais on va trouver différents isotopes qui se différencient par leur nombre de neutrons :
le carbone 12 possède 6 protons et 6 neutrons (c’est l’isotope le plus courant),
le carbone 13 possède 6 protons et 7 neutrons, et constitue 1,1% du carbone terrestre environ,
le carbone 14 possède 6 protons et 8 neutrons, mais il est instable (radioactif). Il se forme dans la haute atmosphère sous l’effet du bombardement des noyaux d’azote par le rayonnement cosmique.
Or comme la masse d’un atome augmente avec le nombre de neutrons, il est facile de déduire de ce qui précède que plus un isotope comporte de neutrons et plus il est lourd. Ce point est capital, parce que dans toute réaction physique (par exemple l’évaporation) ou chimique, la nature a tendance a favoriser les isotopes les plus légers, qui « réagissent » plus vite.
Et surtout, fait capital pour nous, ce « tri » est d’autant plus efficace que la température est basse. Ainsi, a contrario, plus la température est élevée, et plus les isotopes lourds sont présents en grande proportion à l’arrivée. Par exemple, plus il fait chaud à la surface de l’eau, et plus les isotopes lourds de l’oxygène (l’oxygène 17 et l’oxygène 18, l’isotope le plus léger – et le plus abondant – étant l’oxygène 16) sont présents en grande quantité dans la vapeur d’eau.
Ce raisonnement s’applique également à l’hydrogène, qui est le deuxième constituant de l’eau, et qui comporte 2 isotopes « naturels », l’hydrogène « normal » (juste un proton) et le deutérium (un proton et un neutron, donc 2 fois plus lourd !) : plus il fait chaud à la surface de l’eau qui s’évapore, et plus la proportion de deutérium dans la vapeur d’eau est forte.
Il existe des moyens techniques qui permettent de connaître la proportion des différents isotopes d’un élément dans un échantillon analysé. Ces méthodes s’appellent « l’analyse isotopique » et utilisent souvent un appareil qui s’appelle un spectromètre de masse.
Un spectromètre de masse
(pas celui qui sert aux analyses des coraux, mais j’attends une photo décente!) .
Photo prise sur le site de l’Institut de Physique du Globe, Université Paris 6 Jussieu.
  Par ailleurs, pour les isotopes radioactifs dont la proportion décroit avec le temps (le carbone 14 est l’exemple le plus connu, mais il y en a beaucoup d’autres) nous disposons d’une horloge, qui permet d’accéder à une donnée utile pour reconstituer les températures du passé, car moins il y a de cet isotope radioactif dans l’échantillon analysé, et plus il est vieux. Si nous avons la chance de pouvoir connaître quelle était la proportion de cet isotope dans l’échantillon analysé au moment de sa formation, nous pouvons, en mesurant la proportion restante de cet atome radioactif dans l’échantillon, en déduire son âge approximatif. Ce n’est pas sans intérêt !
L’analyse isotopique est donc d’une incomparable utilité pour l’étude des climats du passé.
La glace
Après les coraux et les troncs d’arbres, un troisième matériau qui permet d’obtenir des choses intéressantes est tout simplement la glace, notamment des calottes polaires, mais aussi des glaciers continentaux. Comme il est expliqué dans l’encadré ci-dessus, la proportion des différents isotopes de l’oxygène ou de l’hydrogène dans la glace permet de reconstituer la température qu’il faisait au moment où les précipitations de neige (qui se transformera en glace ensuite) ont eu lieu. En effet, plus il fait chaud à la surface de la terre, plus la proportion d’isotopes lourds (Oxygène 18, deutérium) dans la vapeur d’eau est importante, et donc plus cette proportion est importante dans la neige qui tombe en Antarctique ou au Groenland.
Pratiquement, on va prélever une très longue carotte dans la glace des calottes polaires, et à intervalles réguliers on va prélever un peu de glace dans cette carotte et mesurer la proportion d’isotopes lourds.
Comme l’épaisseur de glace des calottes est très importante, cette méthode permet de remonter très loin dans le temps (plusieurs centaines de milliers d’années en Antarctique), mais bute sur un problème de datation fine des échantillons, car l’âge de la glace analysée ne peut généralement se déduire que d’un modèle d’écoulement de la glace, non de mesures directes effectuées dans cette dernière.
Un trou dans la terre
Une quatrième méthode pour reconstituer la température qu’il a fait consiste tout simplement…. à mesurer directement la température sur toute la hauteur d’un trou de quelques centaines de mètres de profondeur creusé dans la terre (l’expression anglaise est « borehole thermometry »). En effet, la température le long du trou dépend de la température moyenne de surface qu’il a fait dans le passé, et de la vitesse de diffusion de la température de surface vers la profondeur ensuite (tout matériau conduit un peu la chaleur, et donc la température dans les premières centaines de mètres change au cours du temps, en « subissant » les variations de température de surface qui se répercutent lentement en-dessous, qui se superposent, bien sur, aux variations de l’énergie géothermique qui fluctue un peu aussi). Avec des petits calculs appropriés (mais que je serais bien incapables de faire !) il est possible de déduire des renseignements sur l’évolution de la température moyenne de surface au-dessus du trou.
De tels « trous » ont été creusés en 600 endroits différents à la surface de la terre, mais on considère que les indications fournies cessent d’être valables si l’on remonte plus de 5 siècles en arrière, et en outre des modifications de l’occupation des sols induisent des perturbations dans les résultats. Les résultats sont toutefois qualitativement convergents avec ce que l’on trouve ailleurs.
Cette méthode peut aussi être utilisée dans le trou qui se forme quand on extrait une carotte de glace du Groenland ou de l’Antarctique. La température mesurée à diverses profondeurs du trou dépend à la fois de la température de surface qu’il a fait quand la glace s’est formée puis de la manière dont cette température de surface a évolué ensuite, les modifications se répercutant lentement vers les profondeurs.
Les sédiments marins
Pour des périodes encore plus anciennes, on peut également analyser les sédiments marins (voir page sur les courants). Cette méthode permet de remonter le temps sur plusieurs dizaines de millions d’années, et permet toujours de reconstituer des températures ! (voir encadré de la page sur les courants).
Bien sûr, plus on remonte loin, et plus le « pas de temps » (c’est-à-dire l’intervalle minimum qui sépare deux mesures) est important : l’analyse de sédiments marins vieux de quelques millions d’années, par exemple, ne permet pas d’obtenir autre chose que des moyennes sur un ou plusieurs millénaires. De même, l’analyse des glaces polaires ne permet généralement pas une résolution supérieure à quelques années (plus l’époque et récente et plus la précision est grande), ce qui est cependant amplement suffisant pour le propos qui nous concerne.
Et au final, que pouvons-nous en déduire ?
Si nous faisons une compilation de toutes ces méthodes d’analyse, voici ce qu’a probablement donné l’évolution des températures sur l’hémisphère Nord pendant le dernier millénaire.
Evolution probable des températures sur 1000 ans.
La légende en Anglais signifie: « valeurs mesurées avec des thermomètres (en rouge) ou déduites d’analyses de cernes d’arbres, de coraux, de carottes de glace et d’archives diverses (en bleu). La courbe noire est la valeur moyenne (sur l’hémisphère Nord) la plus probable, la zone grise la plage des températures « possibles » (ou encore la marge d’erreur).
 Source: Climate Change, the scientific basis, GIEC, 2001.
Ce graphique illustre 2 faits importants à mon sens :
l’augmentation récente est déjà nettement marquée par rapport à une tendance générale qui était stable, voire en très léger refroidissement,
toutefois l’amplitude de ce réchauffement est du même ordre que l’incertitude liée aux méthodes d’estimation pour les années anciennes, ce qui fait dire à d’aucuns que l’on ne peut pas encore totalement éliminer la possibilité d’une seule variabilité « naturelle ».
Sur ce deuxième point, peut-on tenter d’y voir clair ? Avant toute chose, il est indispensable de comprendre qu’il est paradoxalement plus facile de prédire que la température va augmenter à l’avenir que de garantir que notre espèce porte l’essentiel de la responsabilité pour l’élévation constatée dans le passé. Pour prendre un exemple que d’aucuns trouveront peut-être un peu simpliste, imaginons que nous entrions dans une cuisine où une casserole d’eau tiède se trouve sur le feu. L’eau peut être tiède parce que le feu l’a déjà chauffée, ou parce que quelqu’un l’a mise tiède dans la casserole, ou « un peu des deux mon général ». Mais, même si personne n’est capable de déterminer précisément l’origine de la tiédeur déjà visible, on peut prédire sans se tromper que l’eau va continuer à chauffer si on la laisse sur le feu.
De même pour notre climat, il est désormais considéré comme très vraisemblable que c’est notre espèce qui est à l’origine d’une partie du réchauffement du 20è siècle, notamment à cause de ce qui est exposé dans le paragraphe qui suit, mais à la limite cette question peut rester sans réponse précise sans que cela ne remette en cause la validité de la prédiction d’un réchauffement futur. Il y a en effet une conclusion robuste, qui est que tant que la proportion de CO2 augmente dans l’atmosphère la température augmentera ensuite. La question qui se pose pour le passé est de savoir si cette règle a déjà trouvé un début d’application significatif dans le passé ou ce n’est pas encore le cas.
Et pour finir sur ce point, si l’élévation de température passée a surtout résulté de la variabilité naturelle, c’est plutôt une mauvaise nouvelle : cela signifie que la variabilité naturelle nous pousse actuellement vers le haut, et que, en plus, nous allons « prendre » une élévation de température (et une modification climatique associée) liée à nos émissions de gaz à effet de serre.
Pour en revenir à cette question de la responsabilité dans le réchauffement constaté, voici quels sont les éléments qui incitent à penser que nous y sommes pour quelque chose :
la hausse enregistrée est relativement brutale, notamment celle depuis 1970, alors qu’aucune modification rapide des grands déterminants du climat n’a été constatée pendant cette période (voir évolution naturelle du climat dans le passé).
la « structure » de ce réchauffement est particulière : la température a augmenté plus vite la nuit que le jour, plus vite l’hiver que l’été aux moyennes latitudes, et la cause qui est la plus cohérente avec ces évolutions est une augmentation de l’effet de serre. En effet, l’effet de serre, qui se manifeste en permanence avec une intensité à peu près constante, est proportionnellement plus important la nuit ou l’hiver, quand l’énergie fournie par le Soleil est faible ou nulle. Une augmentation de l’activité solaire, a contrario, engendrerait des élévations de température plus marquées l’été et le jour (alors que ce n’est pas le cas), moments où nous recevons de la lumière.
l’ordre de grandeur de l’élévation de température calculée avec un renforcement de l’effet de serre est le bon, alors qu’avec d’autres causes (soleil par exemple) l’amplitude calculée ne correspond pas à ce que l’on observe.
Ainsi, dès le début 2001, le rapport du GIEC expose qu’aucun des modèles climatiques actuellement utilisé ne sait reproduire la hausse des températures sur la deuxième moitié du 20è siècle sans faire intervenir les émissions d’origine humaine de gaz à effet de serre.
Pour les lecteurs avertis
Téléchargez le dernier rapport du groupe 1 du GIEC sur l’état de la connaissance (en anglais). 