Pour de bonnes ou de mauvaises raisons, les « phénomènes extrêmes » focalisent facilement l’attention quand on s’interroge sur les conséquences possibles du changement climatique. La bonne raison, c’est bien évidemment que ce genre de phénomène, quand et où il survient, peut avoir des effets que l’on met des années – voire des décennies – à effacer. Mais la mauvaise raison, d’une certaine manière, est que les effets les plus meurtriers du changement climatique ne seront pas la conséquence de ces phénomènes « extrêmes », mais bien de ce qui deviendra les conditions « ordinaires » du climat, et à la suite l’évolution de la production agricole, des zones endémiques d’épidémie, et de l’instabilité politique. Car un rapide examen de l’histoire nous montre que les plus grandes causes de mortalité sont, et de très loin, les famines, les maladies, et l’instabilité politique (dont les guerres et les régimes totalitaires sont des conséquences), qui ont bien plus tué, et continuent de tuer bien plus, que les ouragans, tsunamis, et autres tornades.
  Ceci étant dit, si nous en revenons à la question, il est très difficile d’y répondre avec les seuls outils qui existent pour faire de la prospective dans cette affaire, à savoir les modèles de climat. En effet, ces derniers ne permettent pas, pour l’instant, de répondre de manière formelle sur la question des « phénomènes extrêmes », qui sont, par définition, des phénomènes inhabituels, alors que la modélisation est surtout destinée à donner des évolutions moyennes. Cela ne signifie bien entendu absolument pas que le nombre de phénomènes extrêmes ou leur violence n’augmentera pas ; un certain nombre d’indices laissent même penser que nous aurons une augmentation de notre exposition à ce risque. Mais il ne s’agit que d’indices, pas d’une preuve formelle, car la modélisation touche à ses limites sur ce point.
Pourquoi les modèles de climat global ne sont-ils pas capables d’apporter une certitude ?
la modélisation climatique date de quelques dizaines d’années déjà. A l’époque, les phénomènes extrêmes ne préoccupaient absolument pas le public, ni même les scientifiques, qui cherchaient surtout à comprendre comment fonctionne le climat dans ses grandes lignes. Les modèles n’ont donc pas été conçus pour pouvoir travailler spécifiquement sur la question des phénomènes extrêmes, et de toute façon il est difficile de raisonner sur ces phénomènes tant que les grands déterminants du climat ne sont pas explicités !
pour pouvoir prédire avec un degré de confiance acceptable l’évolution de phénomènes dont la taille est de l’ordre de quelques centaines de km de diamètre ou moins (un ouragan fait de 500 à 1.000 km de diamètre, une tornade moins de 1 km, un orage quelques km…), il faut travailler avec un modèle dont la maille (voir modèles) est de quelques km de côté, comme c’est le cas en météorologie. Or les modèles climatiques actuels travaillent avec des mailles de l’ordre de la centaine de km de côté (quelques fois moins sur une zone donnée avec les modèles à maille variable, mais alors c’est que la taille de la maille se met à faire bien plus de la centaine de km « ailleurs »), ce qui est très nettement insuffisant pour analyser avec un bon degré de confiance la fréquence de ce genre de catastrophe.
pour les moyennes, il y a eu de nombreux programmes de comparaison des simulations, et c’est à partir de ces comparaisons des résultats de divers modèles que la communauté scientifique s’est accordée sur les conclusions à tirer. Dans le cas des phénomènes extrêmes, il n’y a pas encore eu ce processus de comparaison de modèles alimentés avec des hypothèses identiques, pour savoir ce que chacun d’entre eux « produisait » comme phénomènes extrêmes et voir si il y avait consensus entre les divers modèles.
Mais quelques indices peuvent quand même être obtenus en analysant les résultats disponibles.
La variabilité
La variabilité est la mesure de la fréquence et de l’intensité avec lesquelles une valeur s’écarte de la moyenne.
Comme son nom l’indique, un phénomène extrême est un phénomène « à l’extrême » des valeurs « normales », ces dernières étant près de la moyenne.
Un phénomène extrême correspond donc au cas où un paramètre au moins du climat (les précipitations, le vent, la température…) est très éloignée de sa valeur moyenne. En hiver, une température de 25 °C sera « extrême », parce que la moyenne est beaucoup plus basse. Météo France parle souvent de températures « très éloignées des normales saisonnières » ou « dans les normales saisonnières »: c’est une autre manière de dire « loin de la moyenne » ou « dans la moyenne », ou encore « extrême » ou « normal ».
En regardant si, au fur et à mesure que l’on avance dans le temps, les valeurs instantanées fournies par l’ordinateur (qui concernent la température, la pression, etc, sur tout un ensemble de points, voir page sur les modèles), se comportent de manière de plus en plus erratique sur la durée de simulation (un siècle, 2 siècles…), et surtout comment elles se comparent à la moyenne calculée en bout de simulation, on peut tenter un début de conclusion sur l’évolution de la variabilité du climat futur.
Si cette variabilité du climat modélisé augmente, cela signifie que le risque existe que le climat réel produise, à l’avenir, de plus en plus de valeurs « inhabituelles », « en dehors des normales », et donc qu’il y aura vraisemblablement plus de phénomènes extrêmes, et/ou de plus grande ampleur.
Pour certains modèles (pas pour tous), on a étudié l’évolution de cette variabilité du climat futur. Les modèles étudiés font tous ressortir une évolution à la hausse de cette variabilité, notamment en ce qui concerne le cycle de l’eau.
Cela signifie donc une possible augmentation :
des inondations,
des sécheresses (et peut-être…des gelées intenses sous nos latitudes).
Certains modèles prédisent également une augmentation des tempêtes (ce qui se matérialise par une occurrence plus fréquente de vitesses de vent élevées). C’est un premier résultat, qui demande à être confirmé par un travail approfondi, mais qui indique quand même que le risque de voir apparaître plus de phénomènes extrêmes existe, sans que l’on puisse nécessairement localiser les endroits qui seront plus particulièrement concernés ou quantifier cette augmentation.
El Niño et La Niña
El Niño et La Niña donnent un exemple intéressant illustrant cette difficulté de localisation et les enchaînements en cascade qui peuvent conduire une simple différence de température à des catastrophes.
En effet, les effets observés d’El Nino résultent d’une variation de quelques degrés seulement (2 ou 3 °C) de la température de surface des eaux du Pacifique Est (en fait c’est une inversion entre les températures du Pacifique Ouest – normalement les plus chaudes – et du Pacifique Est – normalement les plus froides – sous l’effet d’un affaiblissement des Alizés, qui normalement poussent les eaux chaudes à l’Ouest).
Or les conséquences :
se produisent pour partie à des milliers de km du phénomène d’origine,
sont « indolores » ou presque à certains endroits, mais très significatives pour les habitants de certaines zones précises : sécheresses intenses en Indonésie (les dernières ont conduit à de gigantesques incendies, assez prévisibles en pareil cas car dans toute cette affaire il ne faut pas oublier la présence de l’homme), moussons diluviennes sur les Andes – avec glissements de terrain à la clé, disparition temporaire de populations de poissons au large du Pérou, et augmentation des cas de choléra au Bangladesh, entre autres choses.
Quid des ouragans tropicaux ?
Si nous regardons le cas particulier des ouragans tropicaux, l’une des conditions pour qu’un tel phénomène prenne naissance est que la température de surface de l’eau soit supérieure à 27°C (parce que la température conditionne la quantité d’eau qui peut s’évaporer par seconde, et cette quantité doit être supérieure à un certain seuil pour que l’ouragan puisse apparaître). A la première question qui est : la température de surface des océans tropicaux va-t-telle monter, la réponse est « très probablement oui ».
Élévation de température moyenne de la surface de divers bassins océaniques en 2080 (par rapport à l’an 2000) avec une concentration atmosphérique en CO2 qui augmente de 1% par an. NW Pacific signifie « Pacifique Nord Ouest », Atlantic parle de lui-même, et NE Pacific signifie « Pacifique Nord Est ». Les sigles (CCCma, CSIRO, etc) désignent les divers modèles utilisés pour faire la simulation.
Toutes les simulations conduisent à une élévation de 1 à 2 degrés de la température de surface des zones de formation des cyclones.
Source : Knutson et al., Journal of Climate, 2004
Si la zone où cette température minimale est atteinte devient plus étendue, on est ensuite tenté de considérer que cela va avoir une incidence soit sur le nombre, soit sur la violence des ouragans, soit sur les deux. Quid du nombre, pour commencer ? Sur ce point, il nous faudra encore attendre pour savoir si cela monte, car les modèles ne donnent pas d’indication exploitable pour confirmer ou infirmer l’hypothèse d’une hausse pour l’instant (voir tableau plus bas).
Et quid de la violence ? La « pompe » qui met l’ouragan en mouvement est une convection très puissante de la surface – chaude – vers la haute atmosphère – froide (au début de la stratosphère la température de l’air est de l’ordre de -50 °C). Plus la différence de température entre la surface et le début de la stratosphère est importante, et plus la convection est puissante. Or il se trouve que le changement climatique devrait réchauffer la basse atmosphère, mais corrélativement refroidir la haute atmosphère. On peut donc craindre des ouragans plus violents dans le climat modifié, et cette éventualité est par contre considérée comme probable au vu des résultats actuels.
Deux publications scientifiques de 2005 viennent renforcer cette supposition. Le premier d’entre eux vient de l’analyse de l’énergie totale dissipée par les ouragans pendant la totalité de la saison cyclonique. Sur l’Atlantique nord, par exemple, la saison cyclonique va en gros de juin à octobre, et on peut évaluer l’énergie libérée par chaque cyclone pendant sa durée de vie, et en aditionnant l’énergie totale par cyclone on obtient l’énergie libérée par la totalité des cyclones de la saison. Dans les graphiques ci-dessous, cette énergie totale des cyclones de la saison s’appelle le PDI (pour Power Dissipation Index, cochons de scientifiques qui publient toujours en anglais !).
On peut ensuite regarder comment cet index (qui correspond peu ou prou au potentiel de destruction des ouragans, puisque plus ils libèrent d’énergie et plus la destruction est potentiellement importante) évolue au cours du temps, et voir si il y a un lien quelconque avec l’évolution de la température de l’océan tropical. Cette analyse sur l’Atlantique a aussi été conduite sur le Pacifique, et les résultats sont les courbes ci-dessous.
Variation de température moyenne de la surface de l’Atlantique tropical nord de 1930 à 2004, et index de dissipation de puissance sur la même période (Atlantic PDI).
On entend par « Atlantique tropical nord » l’océan compris entre le 6è et le 18è parallèle Nord, et 20 à 60 degrés de longitude ouest ; cette courbe porte le nom de HadSST.
Les deux courbes ont été lissées, normalisées, et la courbe des températures décalée vers le haut pour faciliter l’observation de la corrélation.
L’énergie dissipée au cours de la saison cyclonique a quasiment doublé en 30 ans, et semble étroitement corrélée à l’évolution de la température moyenne de l’Atlantique tropical nord.
Source : Emanuel, Nature, 4 Août 2005
Même chose qu’à gauche, mais sur le bassin Pacifique Ouest.
On entend par « Pacifique Ouest » l’océan compris entre le 5è et le 15è parallèle Nord, et 130 à 180 degrés de longitude est ; cette courbe porte aussi le nom de HadSST).
Source Emanuel, Nature, 4 Août 2005
Si la corrélation observée ne peut être qualifiée de loi (un(e) scientifique parle de « loi » quand il s’agit d’une relation de cause à effet qui est valable en toutes circonstances, et devient donc prédictive, ce qui n’est pas le cas ici), il y a quand même un parallélisme troublant dans les observations.
Un autre élément intéressant est que le nombre de cyclones puissants tend à augmenter proportionnellement au sein de l’ensemble des cyclones.
Nombre de cyclones (ou ouragans, ou typhons, c’est la même chose !) dans le monde, par catégories, selon la période (75/79 signifie de 1975 à 1979). A gauche, en valeur absolue (par exemple il y a eu 40 cyclones de catégories 4 et 5 dans le monde de 1970 à 1974), et à droite en part relative (de 1970 à 1974, les cyclones de catégorie 4 et 5 ont représenté 17% du total). La période étudiée est volontairement restreinte à la période avec des données d’observation par satellite.
Le nombre annuel moyen de cyclones de catégories 4 et 5 a plus que doublé en 30 ans, avec une part dans l’ensemble des cyclones qui est passée de 17% à 35% environ.
Source : Webster et al., Science, 16 septembre 2005
En conséquence, si on ne peut répondre formellement à la question de la fréquence ou de la violence des ouragans à l’avenir, les indices dont nous disposons dans un certain nombre de cas laissent cependant penser que les choses devraient plutôt évoluer dans le mauvais sens, notamment pour la violence, en période de réchauffement du climat. Cela ne préjuge pas de la manière dont les choses se passeront une fois la température stabilisée, mais cela n’est pas pour demain !
Que dit la communauté scientifique, exactement ?
L’état des connaissances sur la survenue, à l’avenir, de phénomènes extrêmes, ainsi que sur l’éventuel lien pouvant exister entre le réchauffement climatique déjà constaté et d’éventuels phénomènes extrêmes récents (comme ceux évoqués ci-dessous) peut être résumé dans ce tableau publié dans le résumé pour décideurs du rapport 2007 du GIEC :
Evolution climatique concernée Cette évolution s'est-elle déjà manifestée durant la deuxième moitié du XXè siècle (après 1960) ? L'homme a-t-il contribué à l'évolution constatée ? Cette évolution est-elle simulée pour le XXIè siècle avec les scénarios d'émission utilisés ?
Moins de jours froids au-dessus de la quasi-totalité des terres émergées Très vraisemblablement oui (P > 90%) Vraisemblablement oui (P > 66%) Virtuellement certain (P > 99%)
Augmentation de la fréquence et de l'intensité des jours chauds au-dessus des continents Très vraisemblablement oui (P > 90%) Vraisemblablement oui (nuits) (P > 66%) Virtuellement certain (P > 99%)
Augmentation des températures maximales, et des vagues de chaleur Vraisemblablement oui (P > 66%) Oui est plus vraisemblable que non (P > 50%) Très vraisemblablement oui (P > 90%)
Augmentation des épisodes pluvieux intenses Vraisemblablement oui (P > 66%) Oui est plus vraisemblable que non (P > 50%) Très vraisemblablement oui (P > 90%)
Augmentation de la surface touchée par les sécheresses Vraisemblablement oui dans de nombreuses régions depuis 1970 (P > 66%) Oui est plus vraisemblable que non (P > 50%) Vraisemblablement oui (P > 66%)
Plus d'activité cyclonique intense aux tropiques Vraisemblablement oui dans de nombreuses régions depuis 1970 (P > 66%) Oui est plus vraisemblable que non (P > 50%) Vraisemblablement oui (P > 66%)
Augmentation du nombre d'épisodes d'élévation extrême du niveau de la mer (hors tsunamis) Vraisemblablement oui (P > 66%) Oui est plus vraisemblable que non (P > 50%) Vraisemblablement oui (P > 66%)
Les tempêtes de l’hiver 1999 en France sont-elles une conséquence du changement climatique ?
Conformément à ce qui précède, il est prématuré de dire que ces tempêtes – en fait des ouragans – sont une conséquence du réchauffement global (l’évolution évaluée ci-dessus concerne les ouragans tropicaux, pas les ouragans des moyennes latitudes). Si nous nous tournons vers le passé pour savoir si il y a une tendance à la hausse du nombre de tempêtes, la réponse est négative pour l’instant :
Nombre de tempêtes observées en France de 1950 à 1999.
Il est difficile de discerner une tendance franche concernant le nombre de tempêtes ou d’ouragans dans ces données. Par contre ce graphique ne dit rien sur la violence des événements, c’est-à-dire la vitesse maximale atteinte pour le vent.
Source : météo France
Comment pourrions nous être sûrs de « quelque chose » ? Nous aurions une certitude si nous pouvions faire l’expérience suivante : 50 fois nous remettons la terre en 1900, injectons des gaz à effet de serre dans l’atmosphère, puis observons ce qui se passe, et ensuite nous remettons 50 fois la terre en 1900, n’injectons pas de gaz à effet de serre supplémentaires dans l’atmosphère, et observons aussi ce qui se passe.
Si les 50 fois où nous avons injecté des gaz à effet de serre nous avons un ouragan en France en décembre 1999, l’affaire est entendue : sans notre action sur le climat, ces ouragans ne seraient pas arrivées, et nous pouvons donc les considérer comme des conséquences du changement climatique provoqué par l’homme.
Mais, bien sûr, il n’est pas possible de remonter le temps ! Personne ne peut donc faire la seule expérience qui permettrait d’avoir une réponse définitive à la question posée en ouverture de ce paragraphe. Tout ce que nous pouvons faire, alors, est de prendre un modèle climatique, de le perfectionner jusqu’à ce qu’il reproduise le climat actuel le mieux possible, puis simuler une augmentation des gaz à effet de serre dans l’atmosphère, et regarder si nous obtenons alors plus d’ouragans que maintenant, ou des ouragans plus violents.
Et jusqu’à maintenant ce genre de simulation n’a pas été faite assez souvent, avec un protocole précis de comparaison des résultats, pour que l’on puisse avoir une réponse acceptable. Tout ce que l’on peut dire est que ce qui s’est passé en 1999 est cohérent avec les indications fournies par la modélisation, en particulier en ce qui concerne la partie « inondations » et « augmentation de l’intensité des ourgans dans leur zone de survenance ».
On peut aussi être troublé par le rapprochement entre les conditions qui ont permis l’apparition de cette « tempête » et ce que prédisent certains modèles.
Le « moteur » des tempêtes est toujours une différence élevée de température ou d’humidité entre deux masses d’air (on parle de gradient). Or plusieurs simulations indiquent que le changement climatique ne devrait pas beaucoup réchauffer (voire refroidir) l’air situé au nord de l’Europe (en fait près du Groenland), par suite du ralentissement de la circulation océanique, mais réchauffer un peu plus l’air situé au sud de l’Europe.
Élévations régionales de température moyenne par rapport à la période 1980-1999 pour la décennie 2020-2029 (à gauche) et pour la décennie 2090-2099 (à droite). Il s’agit d’une moyenne inter-modèles, établie à partir des sorties de 6 à 8 modèles. Ces élévations sont données, de haut en bas, pour trois scénarios différents, B1, A1B et A2, soit en gros des émissions stables, qui doublent, et qui sont multipliées par 4 (voir descriptifs sur la page sur les scénarios).
Les cartes pour 2100 font clairement apparaître une différence de température accrue entre le sud du Groenland et l’Europe.
Source : GIEC, 4è rapport d’évaluation, 2007
  Une telle évolution augmenterait chroniquement la différence de température entre le Nord et le Sud de l’Atlantique.
On se rappellera peut-être que les tempêtes de l’hiver 1999 (qui étaient en fait des ouragans) avaient pour origine la confrontation au -dessus de la France d’une masse d’air inhabituellement chaude venu du sud de l’Atlantique avec une masse d’air inhabituellement froide venu du nord de l’Atlantique,
En conséquence, une évolution régionale telle que celle figurant sur la carte ci-dessus « aiderait probablement le hasard » dans l’apparition de tempêtes analogues à celles que la France a connue.
Plus généralement, les modèles prédisent tous des augmentations de gradient ici ou là, mais pas aux mêmes endroits (ce qui est une conséquence normale de leurs limites). Si une future augmentation du nombre de tempêtes en France ne peut donc être prédite, du moins ce genre de phénomène donne une idée de ce qui pourrait arriver.
Deux derniers points sont d’importance :
si ce genre de phénomène est déjà une conséquence du réchauffement, il y a probablement bien pire à venir, les modifications en cours étant destinées à s’accentuer quoi que nous fassions pendant encore un siècle au moins,
Quand bien même il serait prouvé que les phénomènes extrêmes violents (de type ouragans) ne vont pas augmenter en fréquence ou en intensité, cela ne rend pas pour autant le phénomène du réchauffement climatique exempt de risques !
Les inondations de 2002 en Allemagne sont-elles liées au changement climatique ?
Même motif, même punition : pour en avoir le cœur net, il faudrait voir comment évoluerait une terre qui n’aurait pas reçu de gaz à effet de serre d’origine humaine depuis 1750 : c’est une expérience difficile à faire ! En outre il ne faut pas oublier que pour avoir des dégâts des eaux, il faut à la fois de l’eau et… quelque chose à inonder. Or l’urbanisation a toujours eu lieu le long des cours d’eau (pas une ville ou un village qui ne soit traversée par une rivière !), et les terrains non inondables ayant été utilisés en premier, il en résulte que les constructions neuves, par suite de la pression foncière, sont plus souvent qu’avant construites près de l’eau. De ce fait, à élévation égale du niveau de l’eau, il y a de plus en plus de choses à inonder : quand les dégâts augmentent, faire la part des choses entre ce qui est purement climatique et ce qui découle uniquement du changement d’occupation des sols n’est pas forcément aisé.
Ensuite il n’y a pas de lien simple entre la quantité d’eau qui tombe sur le bassin versant (ce sur quoi il y a des conclusions de disponibles) et le débit du fleuve en aval : la relation dépend de la vitesse à laquelle tombe l’eau, de ce qui recouvre le bassin versant, de la saison (si l’eau tombe sous forme de neige cela change un peu les choses…), des aménagements de la rivière (l’existence de bassins d’épanchement, ou, au contraire, un lit contraint par des digues modifient fortement les débits) ; bref affirmer que les inondations en Allemagne de l’été 2002 ne seraient pas parvenues sans gaz à effet de serre supplémentaires serait bien osé (et aucun scientifique ne s’y risque).
Enfin sans avoir eu besoin de changer le climat mère Nature a quand même inondé Paris en 1910, et les mêmes précipitations en amont aujourd’hui provoqueraient une inondation à peine moins grave (les ouvrages de rétention ont une capacité limitée ; ils ne feraient baisser le niveau de la crue que de 20 à 30 cm) : crierait-on au changement climatique pour autant ?
Ce qui est le signe incontestable d’un changement de climat n’est donc pas tant l’existence de tels phénomènes que le changement de leur fréquence d’apparition. La certitude viendra éventuellement si, au bout d’un siècle, nous constatons que de telles inondations se produisent une fois tous les 10 ans au lieu de survenir une fois par siècle. Mais alors il sera trop tard pour renverser la vapeur. Et sans déroger à la nécessaire prudence, il est quand même possible d’affirmer, vu que le changement climatique devrait se traduire par une intensification du cycle de l’eau sur l’Europe du Nord, que ces inondations donnent une assez bonne idée (plus que pour les tempêtes) de ce qui va probablement arriver sur le Nord de l’Europe, à des degrés divers, à l’avenir (voir tableau ci-dessus). En outre, il est déjà « vraisemblable » qu’au cours des dernières décennies les épisodes pluvieux intenses ont augmenté.
Evolution décennale moyenne, sur la période 1951 – 2003, de la contribution des jours très pluvieux au total des précipitations.
Les zones laissées blanches sont des zones pour lesquelles l’information n’est pas disponible, faute de relevés (ou de relevés en nombre suffisant).
Une augmentation de cette contribution signifie de la pluie qui tombe de manière de plus en plus « concentrée » sur des épisodes intenses, ce qui n’est pas nécessairement une bonne affaire pour la végétation et augmente le risque d’inondations toutes choses égales par ailleurs.
    Même information que le graphique précédent, mais pour l’ensemble des zones « non blanches ».
Le zéro du graphique correspond à la valeur moyenne sur la période 1961-1990 de la contribution des jours très pluvieux à l’ensemble des précipitations, qui est de 22,5% (22,5% de la pluie totale tombe pendant des épisodes dits « très pluvieux », qui sont ainsi désignés parce que la quantité de pluie qui tombe est plus importante que ce qui se produit pour 95% des épisodes pluvieux – désolé pour cette avalanche de chiffres !).
Les barres donnent la valeur pour l’année considérée de l’écart à cette moyenne de 22,5%, en % également. Par exemple, en 2000, les épisodes très pluvieux ont amené 1% de pluie en plus que la moyenne, donc cette année là 23,5% des précipitations ont eu lieu à l’occasion de tels épisodes (sur les zones où les séries longues sont disponibles).
  Source : GIEC, 4è rapport d’évaluation, 2007
Comme pour les ouragans, la réponse concernant l’avenir (aurons nous plus d’épisodes pluvieux intenses, et donc d’inondations ?) est plutôt plus facile à donner que celle sur le passé, car l’influence relative des gaz à effet de serre est plus faible aujourd’hui que ce qu’elle sera probablement plus tard.
Enfin une question similaire est souvent posée sur les précipitations intenses qui sont survenues dans le Sud de la France en 2002 et 2003.
Nombre d’épisodes pluvieux intenses observés dans le Sud-Est de la France entre 1958 et 2002. Un épisode pluvieux intense se définit comme un événement pour lequel il tombe plus de 100 mm d’eau en 24 heures.
Il est ici aussi difficile de discerner une tendance franche sur le nombre, par contre, comme pour les tempêtes, ce graphique ne dit rien sur l’intensité, c’est-à-dire la quantité totale d’eau qui sera tombée.
Source : Météo France
La canicule de l’été 2003 en France est-elle etc… ?
Est-il besoin de préciser la réponse ? Encore une fois, ce n’est pas tant l’existence d’un été très chaud, même si des records ont été battus, qui permet de donner une réponse formelle pour établir un lien entre émissions passés et cette vague de chaleur.
Distribution de la moyenne des températures estivales en Suisse pour les années 1864 à 2003.
Les extrêmes avant 2003 – et depuis l’existence de relevés thermométriques, bien sûr – étaient 1909 (pour l’été le plus frais ; un peu plus de 15° C de moyenne) et 1947 (le plus chaud, un peu moins de 20 °C). On voit que 2003 « enfonce » complètement le record précédent et se retrouve totalement en dehors de la gaussienne, avec plus de 5° d’écart à la moyenne (alors que l’écart type est de moins de 1 °C !) .
Il est donc tentant de penser qu’il y a là une première marque de notre influence passée. Mais seule l’augmentation de la fréquence de ce genre d’événement apportera une réponse définitive, et à ce moment ce sera trop tard pour renverser la tendance, évidemment.
Source : GIEC, 4è rapport d’évaluation, 2007
Par contre, il est vraisemblable que cet épisode donne une bonne idée de ce qui nous attend de toute façon à l’avenir, et il est même vraisemblable que sur ce point précis (les vagues de chaleur) nous aurons bien pire plus tard (voir tableau ci-dessus), et « d’autant pire » que nous émettrons beaucoup de gaz à effet de serre dans les décennies qui viennent. 