L’histoire peut-elle nous aider à comprendre le présent? Si l’on considère les fluctuations climatiques de notre planète et l’amélioration de nos connaissances à ce sujet, alors la réponse est positive. En effet, une nouvelle étude européenne a démontré que quand nous comprendrons ce qu’il s’est passé dans un passé récent, nous parviendrons à améliorer nos connaissances et les réactions appropriées aux variations climatiques. Cette étude a été financée par le projet EURO4M («European reanalysis and observations for monitoring»), qui s’est arrogé presque 4 millions d’euros au titre du domaine Espace du septième programme-cadre de l’UE. Les résultats, publiés dans la revue Climate Research, ont souligné le besoin de récupérer to utes les données enregistrées dans les formats périssables le plus tôt possible.  Deux chercheurs, du Centre de changement climatique de l’Université Rovira i Virgili (URV) en Espagne et de l’unité de recherche climatique de l’University of East Anglia (UEA) au Royaume-Uni respectivement, ont découvert que 80% des données climatiques mondiales au format numérique ne peuvent être utilisés par les chercheurs. Ainsi, seuls 20% des données disponibles peuvent être utilisés pour des évaluations. Le SINC (Scientific Information and News Service) reprend les dires de l’auteur principal de l’étude, Manola Brunet, la responsable du Centre sur le changement climatique de l’URV, et explique que bien que certaines données climatiques en Europe remontent au XVIIème siècle, «moins de 20 % de ces informations enregistrées dans le passé sont disponibles à la communauté scientifique». Mais la situation sur les données européennes n’est pas aussi tragique que pour les autres continents. Les données sur l’Afrique et l’Amérique centrale ne sont pas accessibles simplement parce que les observations climatiques n’ont démarré qu’au milieu du XIXème siècle. «L’incapacité à déchiffrer les archives climatiques entraînerait des problèmes socio-économiques car nous ne sommes pas en mesure de déceler les impacts actuels et futurs du changement climatique et d’un monde plus chaud», explique le Dr Brunet, responsable de l’étude avec Phil Jones, de l’UEA. Outre l’Espagne, seuls le Canada, les Pays-Bas, la Norvège et les États-Unis ont donné un droit partiel aux scientifiques d’accéder aux données climatiques historiques de leurs pays. Tous les autres pays du monde n’accordent pas un tel accès aux scientifiques ou au public dans un sens plus large, malgré les suggestions émises par l’Organisation mondiale météorologique (OMM) les encourageant à changer de position. Les gouvernements devraient adopter une résolution au sein de l’ONU d’ouvrir les données climatiques historiques» si nous voulons être capables d’aborder les défis politiques et juridiques posés par ce problème actuel, commentait le Dr Brunet. Le duo explique que les services climatiques de tous les pays doivent travailler à la conversion des informations climatiques historiques sur papier en format numérique. Ces données se trouvent actuellement dans des archives, bibliothèques et centres de recherche. Ce mode de stockage complique l’accès à ces données. Aussi, un autre problème est que les services météorologiques devraient laisser le public accéder à ces données, et non restreindre leur accès. «L’objectif est d’offrir un service météorologique [au] public, désireux de connaître le temps qu’il fera le jour suivant», fait remarquer le Dr Brunet. L’un des plus gros problèmes est que la science météorologique, qui étudie les conditions atmosphériques caractérisant une région plutôt que sur la prévision du climat, est devenue la grande ‘victime’, car les chercheurs de ce domaine ont du mal à sécuriser des fonds pour la numéralisation, le développement et la standardisation des données. Enfin, les services climatiques assistent un certain nombre de pays, ainsi que le Canada et les États-Unis. Grâce à ces services, les chercheurs sont en mesure d’expliquer et de remettre dans son contexte la vague de chaleur qui a touché l’Europe de l’est en 2010. «Si nous avions accès à toutes les archives historiques, nous pourrions évaluer la fréquence de ces phénomènes à l’avenir avec un degré de certi tude plus élevé», commente le Dr Brunet. Les chercheurs pensent que ce type d’informations pourraient bénéficier la science, la société et l’économie. By Andre Vellino, chercheur au NRC Canada Institute for Scientific and Technical Information et professeur invité à l’École de l’Information de l’Université de Ottawa.   Les données de recherches scientifiques sont sans nul doute un composant central du cycle de vie de la production de la connaissance. D’une part, les données scientifiques sont essentielles à la corroboration (ou la falsification) des théories. Mais aussi, l’accès ouvert à ces données est tout aussi important pour que le processus d e validation scientifique soit mis en œuvre (comme cela a été récemment démontré par la controverse du « ClimateGate » et la récente affaire sur les données de recherche de la cognition chez les primates de Marc Houser). L’accessibilité publique des données permet une relecture ouverte par les pairs et encourage la reproductibilité des résultats. De ceci découle l’importance des pratiques de gestion des données dans les bibliothèques scientifiques du XXIème siècle : le traitement éditorial, l’accès et la préservation des données de recherches scientifiques vont devenir cruciaux pour le futur du discours scientifique. Il est vrai que les institutions de recherches scientifiques à grande échelle gèrent des données de références depuis longtemps. Dans beaucoup de disciplines, les centres de données dans ces institutions ont rassemblé un bon nombre de bases de données contenant les fruits d’années de recherches. Par exemple, GenBank, la base de données de séquences génétiques du National Institutes of Health (NIH), et une collection annotée et globale de toutes les séquences d’ADN accessible publiquement (plus de 150 000 séquences). Toutefois, d’autres types de données rassemb lées par les scientifiques sont soit éphémères soit très dépendants du contexte et ne sont préservés à long terme pour le bénéfice de recherches futures ni par les institutions ni individuellement par les chercheurs. Ceci n’est pas si important pour les données reproductibles (soit expérimentalement, soit par simulation). Mais beaucoup de données, comme celles concernant la concentration du pétrole et sa dissipation dans l’eau du golf du Mexique en 2010, sont uniques et non-reproductibles. Comme je l’ai indiqué dans un billet précédent, l’émergence de méthodes de référencement unique pour les jeux de données comme celles de DOI implémentées par les partenaires deDatacite, permettrait d’aider à résoudre certains problèmes subit par les petits jeux de données orphelins et inaccessibles. La combinaison de politiques de dépôts de données par les agences de financement de la recherche scientifique (comme NSF aux États-Unis et NSERC au Canada) et la reconnaissance par les pairs des universités envers l’effort intellectuel effectuer dans la création de données va, dans le futur proche, augmenter tant le nombre de publications de données que leur référencement pour correspondre à la situation actuelle avec les publications savantes. En parallèle, l’émergence du mouvement de « l’accès ouvert aux données » et d’autres initiatives qui augmentent la disponibilité des données générées par les gouvernement et les institutions gouvernementales (dont la NASA, le NIH, et la Banque mondiale) sont en bonne adéquation avecles principes de l’OCDE [PDF]. On y trouve dans celui-ci, incidemment, une liste longue et convaincante des bénéfices économiques et sociaux qui découle du libre accès des données scientifiques. Les États-Unis, le Royaume-Uni, et l’Australie sont les fers de lance dans l’effort fait pour rendre les données de recherches scientifiques accessibles. Par exemple, aux États-unis, le récentrapport [PDF] du Conseil national de science et technologie (NSTC) rendu au Président Obama détaille une stratégie complète pour promouvoir l’accès aux données numériques et leur préservation. Ses rapports et initiatives montrent qu’il existe un mouvement mondial pour réaliser et articuler les visions de plusieurs organismes concernés par le traitement et l’archive de données qui s’est créé pendant la première décennie du XXIème siècle (voir To Stand the Test of Time [PDF] et Long Lived Scientific Data Collections [PDF]). Au Canada, d’autres rapports similaires comme la Consultation sur l’accès aux données de recherches scientifiques [PDF] et la Stratégie canadienne de l’information numérique soulignent également le besoin d’un conseil national pour la préservation de l’information numérique, y compris les ensembles de données scientifiques. Malgré beaucoup de discussions, les efforts systématiques dans dans la gestion des données scientifiques canadiennes ne sont qu’à leurs premiers stades. Alors que les données dans certains domaines sont bien préservées et bien gérées, comme en sciences de la terre (avec Géogratis) et en astronomie (avec le Canadian Astronomy Data Centre) qui ont des communautés d’utilisateurs scientifiques spécialisés et dont les besoins sont bien compris, les besoins en gestion de données des scientifiques esseulés dans des petits groupes de recherches mal financés sont soit impossible à trouver soit déjà perdu. Un obstacle au traitement éditorial bibliographique effectif des jeux de données est l’absence de normes communes. Il n’y a pour l’instant « aucune règle de publication, de présentation, de citations ou même de catalogue de jeux de données » (OCDE) [Green, T (2009), “We Need Publishing Standards for Datasets and Data Tables”, OECD Publishing White Paper, OECD Publishing] La Passerelle vers les données scientifiques de l’Institut canadien de l’information scientifique et technique (ICIST) ainsi que d’autres sites nationaux (tel que British National Archives of Datasets) qui rassemble des informations à propos des jeux de données, utilisent des normes bibliographiques (tel que Dublin Core) pour représenter les méta-données. L’avantage est que ces normes ne dépendent pas du domaine et sont déjà suffisamment riches pour exprimer les éléments clés dont on a besoin pour archiver et retrouver les données. Toutefois, ces normes de méta-données développées pour la bibliothéconomie traditionnelle, ne sont pas (encore) suffisamment riches pour récupérer complètement la complexité des données scientifiques venant de toutes les disciplines, comme je l’ai fait valoir dans un précédent billet. Lorsqu’on décide de la faisabilité de la création d’un dépôt de données, une des inquiétudes majeures est le coût lié au dépôt, au traitement et à la préservation à long terme des données de recherches. Typiquement, les coûts dépendent de nombreux facteurs incluant la façon dont les phases caractéristiques (planification, acquisition, mise à disposition, organisation, traitement, archivage, stockage, préservation et les services d’accès) sont déployées (voir les rapports du JISC “Keeping Research Data Safe” Part 1 and Part 2). Les coûts associés à différentes collections de données varient aussi considérablement selon la rareté ou la valeur des données et selon exigences pour l’accès au fil du temps. Un point à noter venant des rapports “Keeping Research Data Safe” est que : « les coûts d’archivage des activités (archivage, stockage et préservation, planification et actions) sont une toute petite proportion des coûts globaux et sont très légers comparés aux coûts d’acquisition/traitement et d’accès des données.” En bref, la gestion de bibliothèques de données est critique pour l’avenir de la science et les coûts de la technologie nécessaire pour cette gestion sont la moindre de nos préoccupations. Cet article est une traduction d’un billet publié sur Synthèse par Andre Vellino Illustration FlickR CC : scott ogilvie, hexodus  Pour de plus amples informations, consulter: EURO4M:
http://www.euro4m.eu/ La recherche dans le domaine de l’espace au titre du 7e PC:
http://cordis.europa.eu/fp7/cooperation/espace_en.html Climate Research:
http://www.int-res.com/journals/cr/   By: Lecteur 