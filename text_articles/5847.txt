Saisi de la plainte[1] déposée en septembre 2017 par les villes de San Francisco et Oakland contre cinq compagnies pétrolières[2], le juge Fédéral William Alsup avait ordonné aux parties de lui présenter sous forme d’un didacticiel (tutorial) « les meilleures informations scientifiques disponibles actuellement sur le réchauffement climatique ». Le texte qui suit est une traduction (partielle [3]) du didacticiel établi pour le compte des  compagnies pétrolières par trois scientifiques  réputés : William Happer, Steven E. Koonin, Richard Lindzen [4]. L’original de ce document est  accessible en suivant ce lien. Notre synthèse sur l’état de la science du climat s’articule autour de quatre points : Les preuves que nous produisons à l’appui de chacune de nos déclarations proviennent essentiellement du rapport spécial sur le climat (Climate Science Special Report, CCSR [5]) publié par le gouvernement américain en novembre 2017, ou du cinquième rapport d’évaluation (RE5) du GIEC [6] publié en 2013-2014 ou encore de la littérature scientifique de source primaire [7]. 1. Le climat a toujours changé. Le graphique de droite à la figure 1 ci-dessous (CSSR Figure ES.1) montre le réchauffement de la planète au cours des 130 dernières années, mesuré directement par les instruments en surface. La vignette de gauche montre les changements de l’anomalie [8] de la température de surface en moyenne globale. Cette anomalie de la température moyenne annuelle a augmenté de plus de 0,9 ° C (1,6 ° F) sur la période 1986-2015 par rapport à la moyenne prise 1901-1960. [Les barres verticales rouges indiquent des températures supérieures à la moyenne sur 1901-1960 et les barres bleues indiquent des températures inférieures à cette moyenne sur 1901-1960.]    La carte de droite montre que le réchauffement a été plus fort sur terre dans l’hémisphère Nord et plus important aux pôles. Comme on peut le voir dans d’autres statistiques du CSSR, il y a d’autres manifestations  d’un réchauffement modéré au cours des dernières décennies, notamment la hausse de la chaleur contenue dans les océans, l’élévation du niveau de la mer, le rétrécissement de banquise arctique, la fonte des glaciers, et une atmosphère plus humide. La situation est cependant plus complexe que ce que l’on pourrait déduire de la figure. Un scientifique qui examine le panneau de gauche, est interpellé par plusieurs  éléments. Premièrement, il n’y a pas de barres d’incertitude, une pratique malheureusement courante dans les représentations du changement climatique; il s’avère que les incertitudes sont de 0.2F (0.1C) au cours des dernières décennies, l’incertitude est deux fois plus important dans les premières années des enregistrements. Deuxièmement, une grande partie de la hausse apparemment alarmante de ces dernières années est due à la situation d’El Niño qui a prévalu, comme lors du pic de température de 1998. Enfin, l’augmentation de l’anomalie de température n’est pas régulière à l’échelle de quelques dizaines d’années. Par exemple, les températures ont diminué pendant la période 1940-1970. Comme l’influences humaine n’a été significative qu’à partir des années 1950, ce graphique suggère que le climat est tout à fait capable de varier substantiellement de façon autonome. Pour étayer ce point, voyons le diagramme représentant des observations géologiques de longue période, représenté schématiquement  à la figure 2 [9] qui suit : il montre directement que l’anomalie de température en moyenne globale a changé de façon spectaculaire au cours des 500 derniers millions d’années, qui ne sont que 10% de l’histoire de la Terre! Il y a eu un réchauffement important au cours des 20 000 dernières années (ligne bleue), depuis le début de la fin de la dernière glaciation ; il y a 120 000 ans, une période interglaciaire (l’Eémien) a été de 2°C plus chaude qu’aujourd’hui avec un niveau de la mer 6 m au-dessus du niveau actuel. Au cours des derniers millions d’années, il y a eu une succession de périodes chaudes et froides, largement attribuables aux variations de l’orbite et de l’orientation de la Terre, avec, au cours des 100 derniers millions d’années, des températures encore plus élevées. Les deux points rouges à droite montrent, pour 2050 et pour 2100 les hausses imaginées des températures, hausses attribuées aux influences humaines. Nous discuterons ci-dessous la fiabilité de ces projections.    Même avec les données instrumentales, le réchauffement des quatre dernières décennies n’est pas inhabituel, comme l’illustre le graphique ci-dessous, qui compare deux périodes de 50 ans, au début du 20ème siècle où les influences humaines étaient minimes et les 50 dernières années où les influences humaines ont été beaucoup plus fortes. Il est difficile de les différencier, car la vitesse et l’amplitudes des réchauffements sont comparables. Le graphique de gauche montre les données les plus récentes avec le pic El Nino de 1998 à l’année 42.  2. L’influence humaine sur le climat ne provoque qu’une petite perturbation Pour caractériser et quantifier les influences humaines, nous devons examiner les flux d’énergie intervenant dans le système climatique, comme illustré dans la figure suivante (CSSR Figure 2.1):   Le système climatique de la Terre est un moteur thermique géant qui réfléchit (rétro diffuse) environ 30% de la lumière solaire, absorbe le reste, puis rayonne en infrarouge thermique vers l’espace une quantité de chaleur presque égale, ce qui alimente les vents, les précipitations et les courants océaniques. Notez que les flux naturels d’énergie sont en ordre de grandeur de centaines de Watts par mètre carré (W / m²) et que, comme indiqué dans le coin inférieur gauche, il y aurait un déséquilibre net de 0,6 [0,2- 1,0] W / m² censé réchauffer la planète. Le diagramme de la figure 5 ci-dessous (CSSR Figure 2.3) montre comment l’ influence des activités humaines sur le climat a augmenté depuis 1750.Les unités sont en W/m2, sur le graphique des flux d’énergie de la figure 4 ci-dessus. Le dioxyde de carbone, qui s’accumule dans l’atmosphère en grande partie à cause de l’utilisation de combustibles fossiles, bien que faible par rapport aux flux d’énergie naturelle exerce la plus forte influence sur le réchauffement. Le méthane et d’autres « gaz à effet de serre bien mélangés »[10] (Well Mixed Green House Gaz) sont également importants.   Les plus forts effets refroidissants attribués à l’homme sont ceux des aérosols ; ils sont assez incertains. Les changements du flux solaire au cours de ces 250 dernières années apparaissent, sur cette figure, négligeables.  Le bilan, en bas du diagramme ci-dessus, montre que l’influence humaine totale, actuellement de l’ordre de 2,3 W / m², fait moins de 1% des flux naturels d’énergie qui parcourent le système. Isoler et prédire les effets d’une si petite influence physique dans un système chaotique et «bruité », dont nous n’avons que des observations limitées n’est pas une tâche facile. Non seulement les grands processus de la machine climatique doivent être compris et calculés avec une très grande précision, mais, en plus, il nous faut être sûrs que tous les autres « petits »phénomènes au niveau du 1% des flux d’énergie ont tous été bien pris en compte. 3. Il est impossible d’évaluer dans quelle mesure le faible réchauffement récent est attribuable aux influences humaines Les modèles de circulation générale (GCM) du système climatique sont des outils importants pour attribuer des causes aux changements observés du système climatique. Dans ces modèles, la Terre est couverte par une grille tridimensionnelle de maille typiquement 100 × 100 km dans l’atmosphère et 10 × 10 km sur les océans, avec verticalement, respectivement, de 10 à20 couches et jusqu’à 30 couches. L’air, l’eau, la quantité de mouvement et l’énergie sont -numériquement- transportés d’un point de la grille à un autre selon les lois de base de la physique avec des « forçages [11]» imposés (tels que les effets supposés du soleil ou de la charge de l’air en aérosols) et avec un pas de temps de 30 minutes. Les résultats d’analyses numériques portant sur plusieurs siècles sont comparés et aux propriétés climatiques moyennes et aux valeurs historiques, de façon à valider les modèles.   Cela parait simple en principe, mais la modélisation comporte d’énormes difficultés. Un défi majeur est que de nombreux phénomènes météorologiques importants interviennent à des échelles spatiales très inférieures à la taille des mailles de la grille, citons la topographie, les nuages, les tempêtes ; le modélisateur doit donc, pour construire un modèle complet, faire des hypothèses sur ces processus d’échelle inférieure à la maille de la grille. Par exemple, étant donnés les profils de température et d’humidité de l’atmosphère dans une maille de la grille, dire « quels seront la hauteur, le nombre et le type des nuages ? ». Bien que ces paramétrages à une échelle spatiale inférieure à celle de la maille de la grille puissent se fonder sur des observations de phénomènes météorologiques, leur formulation repose surtout sur l’appréciation subjective des modélisateurs. Les modèles ne sont donc pas, comme on l’entend souvent, « rien que de la physique » puisque leurs paramètres sont « ajustés [12] » pour essayer de reproduire certains aspects du climat observé. Un second problème, majeur, est qu’il n’y a pas de réglage unique qui reproduise les données climatiques historiques. Puisque le refroidissement par aérosol compense le réchauffement dû aux Gaz à Effet de Serre (GES), un modèle avec de faibles sensibilités aux aérosols et aux GES peut reproduire les données [historiques] aussi bien qu’un modèle avec de grandes sensibilités. Par conséquent la sensibilité aux GES est aujourd’hui incertaine d’un facteur trois (et cette incertitude d’un facteur trois persiste depuis quarante ans), augmentant ainsi l’incertitude de toute projection ou prédiction sur les climats futurs. Troisième problème : les modèles doivent reproduire les variations naturelles du système climatique qui, nous l’avons vu, sont comparables aux changements attribués aux effets anthropiques. Les données climatiques montrent clairement des comportements cohérents à des échelles de temps de quelques années, de quelques décennies et de quelques siècles, comportements dont certains relèvent de changements dans les courants océaniques et dans les interactions entre l’océan et l’atmosphère. Comme nous ne connaissons pas l’état de l’océan d’il y a des décennies ou des siècles, il est difficile de choisir correctement le point de départ du modèle.  Et même si cela était possible, il n’y a aucune garantie que le modèle présente la bonne variabilité au bon moment. En dépit de ces difficultés, le GIEC continue de faire la moyenne de quelque 50 modèles différents provenant de différents groupes de recherche à travers le monde. Ces modèles donnent des résultats très différents les uns des autres par rapport aux observations sur les échelles nécessaires pour mesurer la réponse aux influences humaines. Cette prolifération de modèles divergents est une preuve supplémentaire qu’ils ne sont pas «simplement de la physique».   La figure 7 ci-dessus (cercle rouge ajouté) compare les anomalies de la température moyenne mondiale observées (à partir de trois ensembles de donnée) aux résultats de l’ensemble de modèles CMIP5 (projet de comparaison de résultats de modèles climatique, version 5). La courbe orange épaisse est la moyenne, année par année, des résultats de calculs faits avec ces 36 modèles de l’ensemble CMIP5 ; la zone coloriée en orange délimite la plage de valeurs à ± 2 fois l’écart type. Les lignes en tiretés au-dessus et en dessous sont les valeurs extrêmes, année par année, des anomalies calculées. Toutes les séries temporelles se réfèrent à la valeur la période 1901-1960. Notons que si les modèles font un assez bon travail de reproduction pour les dernières décennies, il échoue entièrement pour la période 1910-1940 (cercle rouge) où les données montrent un réchauffement dont le un taux est plusieurs fois supérieur à la moyenne du modèle. Des comparaisons similaires de modèles de données pour d’autres variables climatiques, à la fois mondiales et régionales, montrent également leurs propres problèmes spécifiques. En effet, comme le mentionne le CSSR  (Climate Science Special Report),  à la page 58) en discutant du rôle des influences humaines sur le climat : « Les principales incertitudes résiduelles concernent l’ampleur et la nature précises des changements aux échelles globales, et surtout régionales, en particulier pour les événements extrêmes et notre capacité à observer ces changements avec une résolution suffisante et à simuler et attribuer ces changements en utilisant des modèles climatiques. » 4. Pour les variables climatiques les plus importantes aucun changement préjudiciable n’a été observé Voici ce que dit le GIEC[13] au sujet de divers événements météorologiques extrêmes observés au cours des dernières décennies. Cette énumération ne constitue un choix de type « cherry picking [14]», car chacun des alinéas ci-dessous est une modeste reformulation du texte qui, dans le rapport AR5 (WGI, Chapitre 2), résume la discussion d’un phénomène météorologique particulier. «  … il est très vraisemblable [15]que, depuis 1950 environ, le nombre de jours froids et de nuits froides a diminué et que le nombre de jours chauds et de nuits chaudes a augmenté … on n’a qu’une confiance moyenne [16]dans l’affirmation que la durée et la fréquence d’épisodes chauds, dont les vagues de chaleur, aurait augmenté depuis le milieu du 20ème siècle … il est vraisemblable[17] que depuis 1951, plus de régions ont vu un accroissement du nombre d’épisodes de fortes précipitations que de régions qui ont vu une diminution de ce nombre, mais il existe d’importantes variations régionales et sous-régionales … on a une faible confiance[18] quant au signe de l’évolution de l’ampleur et/ou de la fréquence des inondations, à l’échelle mondiale. … on a une faible confiance quant à une tendance des sécheresses ou de l’aridité, depuis le milieu de 20ème siècle, à l’échelle mondiale, … on a une faible confiance quant aux tendances pour les phénomènes météorologiques de petite échelle tels que la grêle et les orages, … on a une faible confiance dans une quelconque augmentation de long terme (sur cent ans) de l’activité des cyclones tropicaux, … Pratiquement[19] certaine serait une augmentation de la fréquence et de l’intensité des cyclones tropicaux les plus forts dans l’Atlantique Nord depuis les années 1970 … on a une faible confiance dans l’existence de changements à grande échelle de l’intensité des cyclones extratropicaux extrêmes depuis 1900. » Contrairement à l’impression que donnent la plupart des annonces des médias et des discussions politiques, les données historiques d’observation (et l’évaluation qui en est faite par le GIEC) ne suggèrent en aucune manière que des phénomènes météorologiques extrêmes seraient, globalement, devenu plus courants. La plus catégorique des affirmations du GIEC sur des phénomènes météorologiques extrêmes vise les températures, et même là, ça n’est pas simple. Voyons la figure ci-dessous (CSSR, Figure 6.3) carte des températures extrêmes aux États-Unis d’Amérique. Même si le territoire des États-Unis entre les frontières mexicaine et canadienne[20], ne fait que 1,6% de la surface de la Terre, c’est l’une des régions avec la plus forte densité de stations météorologiques et les plus longues séries de mesures.    Les cartes (vignettes en haut de la figure) montrent les “changements” station météo par station météo ; par changement on entend ici la différence entre la moyenne prise sur 1986-2016 et la moyenne prise sur 1901-1960. Les séries temporelles (vignettes en bas de la figure) montrent la moyenne de ces “changements” pondérés par la surface affectée à la station météo, pour les “48 états” des États-Unis. Ne sont retenues, dans les séries de mesures journalières du Global Historical Climatology Network (Menne et al. 2012), que les stations avec des séries longues et très peu de données manquantes. Les températures les plus froides ont crû, mais pas les températures les plus chaudes, qui sont devenues plus fraîches dans la moitié Est du pays. Dans l’ensemble, le climat moyen devient « plus doux » dans la plupart des États-Unis. Des travaux très récents [21] attribuent l’absence de hausse des températures dans la moitié Est du pays à l’intensification des activités agricoles: une croissance végétale plus dense avec des niveaux de CO2 plus élevés dégage plus d’humidité, ce qui à la fois refroidit l’air et augmente les précipitations. Comme l’indique le graphique ci-dessous, le niveau de la mer a commencé à monter il y a quelques 20 000 ans, à la fin du dernier maximum glaciaire. Il est monté de 120 mètres entre 20 000 ans avant le présent et 7000 ans avant le présent, puis il a très considérablement ralenti.   Pour apprécier au mieux si des influences humaines causent une accélération de la montée du niveau des mers, nous devons examiner les données observées pendant le siècle dernier sur des marégraphes répartis sur tout le globe. Trois analyses sont montrées dans la vignette de gauche [22] de la figure 10 ci-dessous (Ces analyses doivent corriger les données brutes de la montée ou descente du rocher qui porte le marégraphe, selon le mouvement tectonique de la côte propre à chaque site).   Ces données montrent que le niveau “global” de la mer a monté de quelque 200 mm depuis 1900, soit en moyenne 1,8 mm / an, mais avec une variabilité considérable à l’échelle décennale. Le niveau de la mer est aussi mesuré par altimétrie depuis des satellites, depuis 1993 ; la détection d’une accélération avec des enregistrements portant sur une période aussi courte est controversée. La signature d’un effet de l’homme sur le niveau des mers devrait être une montée plus rapide après 1950, date à partir de laquelle les influences humaines [23] ont commencé à devenir appréciables. Cette signature ne se voit pas sur la montée annuelle pendant le siècle dernier, vignette de droite de la figure 10 ci-dessus, tirée de la référence citée par le CSSR [24]; l’accélération après 1990 n’est pas statistiquement différente de celle, présumée naturelle, des années 1930. Vu les variations observées avant 1950 et le quadruplement des influences humaines depuis 1950, il faut conclure que l’élévation du niveau des mers est due à des facteurs importants autres que le CO2. Les prévisions du « consensus »sur la montée globale du niveau des mers d’ici 2100 sont très nettement en opposition avec les observations faites site par site. La figure ci-dessous montre les moyennes mensuelles de l’enregistrement par la NOAA [25] du niveau de la mer (corrigé des variations saisonnières) observé à la station de San Francisco [26].  En dehors des décalages dus aux déplacements de la station [27], la série montre une augmentation constante de quelque 2 mm / an. Pour arriver à +1 mètre d’ici 2100, moyenne des projections du GIEC, le niveau de la mer devrait, sur le reste du XXIème siècle, monter six fois plus rapidement, à +12 mm / an en moyenne : c’est la pente du trait de couleur verte.  Les tempêtes dites « ouragans » dans l’Atlantique et « typhons » dans le Pacifique sont un autre phénomène qui inquiète. Le graphique figure 12 ci-dessous résume des données [28] sur le nombre et la force de ces tempêtes, y compris celles de la saison 2017 en Atlantique Nord (même ainsi, les ouragans Harvey et Irma [29] ne figurent pas parmi les 10 ouragans les plus intenses). Le graphique du haut “fréquence des ouragans” est la somme courante sur 12 mois de la Fréquence Globale des Ouragans (Global Hurricane Frequency), pour toutes les tempêtes, et pour les seules tempêtes majeures. La série temporelle “All” est le nombre de cyclones tropicaux qui ont atteint au moins la force d’un ouragan (la vitesse maximale du vent y dépasse 64 nœuds). La série temporelle “Major” est le nombre de cyclones tropicaux qui ont atteint au moins la force d’un ouragan majeur (la vitesse maximale du vent y dépasse 96 nœuds). Figure 12 : nombre et la force des tempêtes tropicales 1970-2018 (Source Ryan N. Maue, Weather BELL) Le diagramme du dessous à la figure 12 “activité” des ouragans” montre pour les cinq dernières décennies l’énergie cyclonique accumulée (Accumulated Cyclone Energy, ACE) lissée sur une période de 24 mois, pour tout le globe (courbe du dessus) et dans le seul l’hémisphère nord. L’ACE est une mesure agrégée de l’intensité des tempêtes : chaque tempête y est pondérée par le carré de la vitesse du vent. Noter que l’année indiquée correspond à la valeur de ACE au cours des 24 mois précédents pour l’hémisphère nord (ligne du bas, courbe noire à petits carrés gris) et sur tout le globe (ligne du haut, courbe bleue à petits carrés bleus). La zone entre les deux courbes représente l’ACE total de l’hémisphère sud. Malgré leur très forte variabilité à l’échelle de quelques années, ces données ne montrent pas de tendance. Le Laboratoire de dynamique des fluides géophysiques de la NOAA a mis en ligne [30] au printemps 2016 le constat suivant : « Il est prématuré de conclure que les activités humaines, et en particulier les émissions de gaz à effet de serre qui causent le réchauffement climatique global) ont eu un impact décelable sur l’activité des ouragans atlantiques ou l’activité des cyclones tropicaux à l’échelle globale. » Résumé de la synthèse Les données des observations historiques et celles des observations géologiques suggèrent que les changements récents du climat pendant le siècle dernier restent dans les limites de la variabilité naturelle. Les influences humaines sur le climat (en grande partie, l’accumulation de CO2 venant de combustibles fossiles) sont une action physiquement petite (1%) sur un système complexe, chaotique, multi-processus et multi-échelle. Malheureusement, les données et notre compréhension sont insuffisantes pour quantifier de façon utile la réponse du climat aux influences humaines. Et même si les influences humaines ont quadruplé depuis 1950, les phénomènes météorologiques extrêmes et la montée du niveau des mers ne montrent pas de tendance significative qui puisse leur être attribuée. Les projections des climats futurs et de phénomènes météorologiques extrêmes ne reposent que sur des modèles manifestement inadaptés à cette fin. En conséquence, l’augmentation des niveaux de CO2 ne constitue à l’évidence pas une menace immédiate pour le climat de la planète, et encore moins une menace imminente. [1] NdT Les deux localités californiennes accusent les cinq compagnies d’avoir caché qu’elles savaient depuis longtemps que la combustion des hydrocarbures présente de sérieux risques pour le climat. Elles réclament aux compagnies pétrolières des dédommagements pour financer les coûts d’infrastructures encourus du fait de l’augmentation du niveau de la mer. [2] NdT Chevron, ExxonMobil, ConocoPhillips, BP et Royal Dutch Shell [3] NdT Le didacticiel comporte trois sections : Climate science overview (section1), Answers to specific questions (section 2), Biographies (section 3). Le présent article est la traduction de la première section. [4] NdT William Happer est physicien spécialisé dans la physique atomique, l’optique et la spectroscopie. Il est professeur émérite à l’Université de Princeton, et membre du groupe consultatif JASON[4]. William Happer a été directeur du Bureau des sciences du Département de l’énergie pendant l’administration de George HW Bush. Steven E. Koonin est physicien, Directeur du Centre pour la science urbaine et le progrès à l’Université de New York.  Il a été sous-secrétaire aux sciences au Département américain de l’énergie de mai 2009 à novembre 2011. Richard S. Lindzen, est physicien, ancien professeur de météorologie au Massachusetts Institute of Technology. Il a publié plus de 200 papiers et ouvrages scientifiques. Il est l’un des principaux auteurs du chapitre 7, “Processus climatiques physiques et rétroactions”, du troisième rapport d’évaluation du GIEC sur le changement climatique. Climatosceptique, il dénonce l’alarmisme en matière de réchauffement climatique [5] https://science2017.globalchange.gov/ [6] https://ipcc.ch/report/ar5/ [7] NdT Les articles de sources primaire sont ceux qui ont été écrits par le chercheur qui a mené l’expérience ou recueilli les données. [8] NdT : l’anomalie est définie comme la différence de la série observée à la moyenne prise sur une partie de cette série [9] https://en.wikipedia.org/wiki/File:All_palaeotemps.png#Summary [10] NdT Well-Mixed Greenhouse Gases (IPCC – https://www.ipcc.ch/ipccreports/tar/wg1/218.htm) [11] NdT En climatologie, le forçage radiatif est approximativement défini comme la différence entre l’énergie radiative reçue et l’énergie radiative émise par un système climatique donné. Un forçage radiatif positif tend à réchauffer le système, alors qu’un forçage radiatif négatif va dans le sens d’un refroidissement (https://fr.wikipedia.org/wiki/For%C3%A7age_radiatif) [12] https://www.geosci-model-dev.net/10/3207/2017/ [13] NdT Voir la terminologie utilisée par le GIEC pour quantifier le niveau de confiance et les probabilités (https://www.ipcc.ch/publications_and_data/ar4/wg1/en/ch1s1-6.html) [14] Cherry Picking (Littéalement  cueillette cerise) : choisir les seules études ou données qui confortent une thèse [15] NdT Very likely : Probabilité supérieure à 90% [16] Ndr Medium confidence : 5 chances sur 10 [17] NdT Likely : Probabilité supérieure à 66% [18] NdT Low confidence : à peu près 2 chances sur 10 [19] NdT Virtually certain dans le texte anglais  (probabilité supérieure à 99% [20] NdT Désigné par «les 48 états » [21] https://agupubs.onlinelibrary.wiley.com/doi/full/10.1002/2017GL075604 [22] http://rsta.royalsocietypublishing.org/content/372/2025/20130336 [23] Lorsque les quantités de dioxyde de carbone venant de la combustion de charbon, pétrole et gaz naturel] [24] https://www.nature.com/articles/nature14093 [25] https://tidesandcurrents.noaa.gov/sltrends/sltrends_station.shtml?stnid=9414290 [26] pourvu de marégraphes depuis 1854 [27] [NdT :  marégraphe à Fort Point pour 1854-1877, à Sausilito pour 1878-1897, et à Presidio Park depuis 1889] [28] http://policlimate.com/tropical/ [29] https://en.wikipedia.org/wiki/List_of_Atlantic_hurricane_records [30] https://www.gfdl.noaa.gov/global-warming-and-hurricanes/                             